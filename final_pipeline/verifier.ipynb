{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1109fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from run import symbol_detection_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb03cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load templates and reference colors\n",
    "pickle_path = \"data/templates.pkl\"\n",
    "\n",
    "with open(pickle_path, \"rb\") as f:\n",
    "    reference_colors, all_templates = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(all_templates)} templates\")\n",
    "print(f\"Reference colors shape: {np.array(reference_colors).shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth\n",
    "with open(\"data/groundtruth.json\", \"r\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "print(f\"Ground truth loaded for {len(ground_truth)} images\")\n",
    "print(\"Sample ground truth:\", dict(list(ground_truth.items())[:3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of test images\n",
    "image_dir = \"data/images/\"\n",
    "test_images = [f for f in os.listdir(image_dir) if f.endswith(('.jpeg', '.jpg', '.png'))]\n",
    "test_images.sort()\n",
    "\n",
    "print(f\"Found {len(test_images)} test images\")\n",
    "print(\"Test images:\", test_images[:5], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7080fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids (all except ncc_threshold)\n",
    "param_grid = {\n",
    "    'rgb_threshold': [0.4], \n",
    "    'black_val_threshold': [50], \n",
    "    'white_sat_threshold': [0.1], \n",
    "    'white_val_threshold': [100], \n",
    "    'min_line_length_percent': [0.05], \n",
    "    'adjacency_radius': [5], \n",
    "    'dedup_thresh': [100], \n",
    "    'scale': [2],\n",
    "    'num_octaves': [4], \n",
    "    'angle_tolerance': [3], \n",
    "    'distance_tolerance': [3] \n",
    "}\n",
    "\n",
    "# NCC threshold range \n",
    "ncc_thresholds = np.arange(0.75, 0.96, 0.05) \n",
    "\n",
    "print(\"Parameter grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "print(f\"\\nNCC thresholds: {ncc_thresholds}\")\n",
    "\n",
    "# Calculate total number of experiments\n",
    "total_combinations = 1\n",
    "for values in param_grid.values():\n",
    "    total_combinations *= len(values)\n",
    "print(f\"\\nTotal parameter combinations: {total_combinations}\")\n",
    "print(f\"Total experiments (with NCC variations): {total_combinations * len(ncc_thresholds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbefe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract template names from results\n",
    "def extract_template_names(results):\n",
    "    \"\"\"Extract template names from pipeline results\"\"\"\n",
    "    if 'matches' not in results:\n",
    "        return []\n",
    "    return [match['template_name'] for match in results['matches']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute precision and recall for a single image\n",
    "def compute_precision_recall(predicted_templates, ground_truth_templates):\n",
    "    \"\"\"Compute precision and recall for a single image\"\"\"\n",
    "    if not predicted_templates and not ground_truth_templates:\n",
    "        return 1.0, 1.0 \n",
    "    if not predicted_templates:\n",
    "        return 1.0, 0.0 \n",
    "    if not ground_truth_templates:\n",
    "        return 0.0, 1.0 \n",
    "    \n",
    "    predicted_set = set(predicted_templates)\n",
    "    ground_truth_set = set(ground_truth_templates)\n",
    "    \n",
    "    true_positives = len(predicted_set.intersection(ground_truth_set))\n",
    "    \n",
    "    precision = true_positives / len(predicted_set) if predicted_set else 0.0\n",
    "    recall = true_positives / len(ground_truth_set) if ground_truth_set else 0.0\n",
    "    \n",
    "    return precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fcd403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate a single experiment across all NCC thresholds\n",
    "def evaluate_experiment(params, experiment_id):\n",
    "    \"\"\"Evaluate a single parameter combination across all NCC thresholds\"\"\"\n",
    "    print(f\"\\n=== Experiment {experiment_id} ===\")\n",
    "    print(f\"Parameters: {params}\")\n",
    "    \n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    \n",
    "    for ncc_idx, ncc_threshold in enumerate(ncc_thresholds):\n",
    "        print(f\"  NCC threshold {ncc_idx+1}/{len(ncc_thresholds)}: {ncc_threshold:.2f}\")\n",
    "        \n",
    "       \n",
    "        image_precisions = []\n",
    "        image_recalls = []\n",
    "        \n",
    "        for img_idx, image_name in enumerate(test_images):\n",
    "            if img_idx % 5 == 0: \n",
    "                print(f\"    Processing image {img_idx+1}/{len(test_images)}: {image_name}\")\n",
    "            \n",
    "            image_path = os.path.join(image_dir, image_name)\n",
    "            ground_truth_templates = ground_truth.get(image_name, [])\n",
    "            \n",
    "            try:\n",
    "               \n",
    "                results = symbol_detection_pipeline(\n",
    "                    image_path=image_path,\n",
    "                    templates=all_templates,\n",
    "                    reference_colors=reference_colors,\n",
    "                    ncc_threshold=ncc_threshold,\n",
    "                    **params\n",
    "                )\n",
    "                \n",
    "                predicted_templates = extract_template_names(results)\n",
    "                precision, recall = compute_precision_recall(predicted_templates, ground_truth_templates)\n",
    "                \n",
    "                image_precisions.append(precision)\n",
    "                image_recalls.append(recall)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error processing {image_name}: {e}\")\n",
    "               \n",
    "                image_precisions.append(0.0)\n",
    "                image_recalls.append(0.0)\n",
    "        \n",
    "       \n",
    "        avg_precision = np.mean(image_precisions)\n",
    "        avg_recall = np.mean(image_recalls)\n",
    "        \n",
    "        precision_scores.append(avg_precision)\n",
    "        recall_scores.append(avg_recall)\n",
    "        \n",
    "        print(f\"    Avg Precision: {avg_precision:.3f}, Avg Recall: {avg_recall:.3f}\")\n",
    "    \n",
    "   \n",
    "   \n",
    "    sorted_indices = np.argsort(recall_scores)\n",
    "    sorted_recalls = np.array(recall_scores)[sorted_indices]\n",
    "    sorted_precisions = np.array(precision_scores)[sorted_indices]\n",
    "    \n",
    "    auprc = auc(sorted_recalls, sorted_precisions)\n",
    "    \n",
    "    print(f\"  AUPRC: {auprc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'experiment_id': experiment_id,\n",
    "        'parameters': params,\n",
    "        'ncc_thresholds': ncc_thresholds.tolist(),\n",
    "        'precision_scores': precision_scores,\n",
    "        'recall_scores': recall_scores,\n",
    "        'auprc': auprc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18943514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main grid search execution\n",
    "print(\"Starting grid search...\")\n",
    "print(f\"Total combinations to evaluate: {total_combinations}\")\n",
    "\n",
    "results_list = []\n",
    "experiment_id = 0\n",
    "\n",
    "# Generate all parameter combinations\n",
    "param_names = list(param_grid.keys())\n",
    "param_values = list(param_grid.values())\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for combination in tqdm(list(product(*param_values)), total=total_combinations, desc=\"Grid Search Experiments\"):\n",
    "    experiment_id += 1\n",
    "\n",
    "    params = dict(zip(param_names, combination))\n",
    "\n",
    "    experiment_result = evaluate_experiment(params, experiment_id)\n",
    "    results_list.append(experiment_result)\n",
    "\n",
    "print(f\"\\nGrid search completed! Evaluated {len(results_list)} experiments.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f58ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "print(\"\\n=== RESULTS SUMMARY ===\")\n",
    "\n",
    "# Sort results by AUPRC\n",
    "sorted_results = sorted(results_list, key=lambda x: x['auprc'], reverse=True)\n",
    "\n",
    "print(f\"\\nTop 5 experiments by AUPRC:\")\n",
    "for i, result in enumerate(sorted_results[:5]):\n",
    "    print(f\"{i+1}. Experiment {result['experiment_id']}: AUPRC = {result['auprc']:.4f}\")\n",
    "    print(f\"   Parameters: {result['parameters']}\")\n",
    "\n",
    "print(f\"\\nWorst 5 experiments by AUPRC:\")\n",
    "for i, result in enumerate(sorted_results[-5:]):\n",
    "    print(f\"{len(sorted_results)-4+i}. Experiment {result['experiment_id']}: AUPRC = {result['auprc']:.4f}\")\n",
    "    print(f\"   Parameters: {result['parameters']}\")\n",
    "\n",
    "# Statistics\n",
    "auprc_scores = [r['auprc'] for r in results_list]\n",
    "print(f\"\\nAUPRC Statistics:\")\n",
    "print(f\"  Mean: {np.mean(auprc_scores):.4f}\")\n",
    "print(f\"  Std:  {np.std(auprc_scores):.4f}\")\n",
    "print(f\"  Min:  {np.min(auprc_scores):.4f}\")\n",
    "print(f\"  Max:  {np.max(auprc_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22729fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "output_file = \"grid_search_results.json\"\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results_list, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {output_file}\")\n",
    "print(f\"Total experiments saved: {len(results_list)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
